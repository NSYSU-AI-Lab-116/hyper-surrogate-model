# Enhanced LLM Model 

![Python](https://img.shields.io/badge/python-3.8+-blue.svg)
![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-orange.svg)
![License](https://img.shields.io/badge/license-MIT-green.svg)

ä¸€å€‹åŸºæ–¼ Gemma-3-270m-it çš„å¢å¼·å‹èªè¨€æ¨¡å‹å¥—ä»¶ï¼Œæ”¯æ´åˆ†é¡ä»»å‹™å’Œæ–‡å­—ç”Ÿæˆï¼Œä½¿ç”¨ LoRA æŠ€è¡“é€²è¡Œé«˜æ•ˆå¾®èª¿ã€‚

## ğŸ¯ å°ˆæ¡ˆæ¦‚è¿°

Enhanced LLM Model æ˜¯ä¸€å€‹å®Œæ•´çš„èªè¨€æ¨¡å‹è¨“ç·´å’Œéƒ¨ç½²è§£æ±ºæ–¹æ¡ˆï¼Œæ»¿è¶³ä»¥ä¸‹æ ¸å¿ƒéœ€æ±‚ï¼š

1. **æ¨¡å‹ A**: åŸºæ–¼ Gemma-3-270m-it çš„ LLMï¼Œå¯ç›´æ¥è¼¸å…¥æ–‡å­—
2. **æ™ºæ…§æç¤ºå·¥ç¨‹**: é‡å°é ˜åŸŸè³‡æ–™é›†çš„æç¤ºå„ªåŒ–ï¼Œæå‡æ¨¡å‹ç†è§£èƒ½åŠ›
3. **åˆ†é¡é ­æ“´å±•**: åœ¨æ¨¡å‹æœ«ç«¯æ·»åŠ å…©å±¤å…¨é€£æ¥å±¤ï¼Œè¼¸å‡º 12 ç¶­åº¦åˆ†é¡çµæœ
4. **å®Œæ•´ä»‹é¢**: æä¾›è¨“ç·´å’Œå›é¥‹ä»‹é¢ï¼Œæ”¯æ´æŒçºŒæ”¹é€²
5. **æ¨™æº–åŒ–ä»£ç¢¼**: éµå¾ªå®˜æ–¹ä»£ç¢¼é¢¨æ ¼ï¼ŒåŒ…å«å®Œæ•´æ–‡æª”å’Œä½¿ç”¨æŒ‡å—

## ğŸ—ï¸ æ¶æ§‹è¨­è¨ˆ

### æ¨¡å‹æ¶æ§‹
```
Gemma-3-270m-it (åŸºç¤æ¨¡å‹)
    â†“
LoRA é©é…å™¨ (é«˜æ•ˆå¾®èª¿)
    â†“
å¹³å‡æ± åŒ–å±¤
    â†“
ç¬¬ä¸€å±¤å…¨é€£æ¥ (hidden_size=256) + ReLU + Dropout
    â†“
ç¬¬äºŒå±¤å…¨é€£æ¥ (output_size=12)
```

### ç³»çµ±çµ„ä»¶
- **model.py**: æ ¸å¿ƒæ¨¡å‹é¡åˆ¥ï¼ŒåŒ…å« EnhancedLLMModel å’Œ TextGenerationModel
- **dataset.py**: è³‡æ–™é›†è™•ç†å’Œæç¤ºå·¥ç¨‹
- **trainer.py**: è¨“ç·´ä»‹é¢å’Œè¶…åƒæ•¸èª¿å„ª
- **evaluator.py**: è©•ä¼°å’Œå›é¥‹ä»‹é¢
- **utils.py**: å¯¦ç”¨å·¥å…·å‡½æ•¸

## ğŸš€ å¿«é€Ÿé–‹å§‹

### å®‰è£ä¾è³´

```bash
# å…‹éš†å°ˆæ¡ˆ
git clone <your-repo-url>
cd hypersurrogatemodel

# å®‰è£ä¾è³´
pip install -r requirements.txt

# æˆ–ä½¿ç”¨ uv (æ¨è–¦)
uv sync
```

### åŸºæœ¬ä½¿ç”¨

#### 1. åˆ†é¡ä»»å‹™è¨“ç·´

```python
from hypersurrogatemodel import (
    EnhancedLLMModel, 
    DomainDatasetProcessor, 
    ClassificationTrainer
)

# åˆå§‹åŒ–æ¨¡å‹ (12 ç¶­åº¦è¼¸å‡º)
model = EnhancedLLMModel(
    base_model_name="google/gemma-3-270m-it",
    num_classes=12,
    hidden_size=256,
    dropout_rate=0.1,
    use_lora=True,
)

# æº–å‚™è³‡æ–™é›†
tokenizer = model.get_tokenizer()
processor = DomainDatasetProcessor(tokenizer)

# å‰µå»ºè¨“ç·´è³‡æ–™
dataset = processor.create_classification_dataset(
    texts=["your", "training", "texts"],
    labels=[0, 1, 2],  # 0-11 çš„æ¨™ç±¤
    domain="your_domain",
    include_prompt=True,  # å•Ÿç”¨æç¤ºå·¥ç¨‹
)

# è¨“ç·´æ¨¡å‹
trainer = ClassificationTrainer(model=model, tokenizer=tokenizer)
results = trainer.train(train_dataset=dataset)
```

#### 2. æ¨¡å‹è©•ä¼°å’Œå›é¥‹

```python
from hypersurrogatemodel import ModelEvaluator, FeedbackCollector

# è©•ä¼°æ¨¡å‹
evaluator = ModelEvaluator(model, tokenizer, class_names)
eval_results = evaluator.evaluate_classification(test_data)

# æ”¶é›†å›é¥‹
feedback_collector = FeedbackCollector()
feedback_id = feedback_collector.collect_classification_feedback(
    text="sample text",
    predicted_label=5,
    correct_label=3,
    confidence=0.85,
    comments="éœ€è¦æ”¹é€²çš„åœ°æ–¹"
)
```

#### 3. å®Œæ•´è¨“ç·´æµç¨‹

```python
from hypersurrogatemodel import TrainingManager

# ä½¿ç”¨è¨“ç·´ç®¡ç†å™¨é€²è¡Œç«¯åˆ°ç«¯è¨“ç·´
manager = TrainingManager(base_model_name="google/gemma-3-270m-it")

results = manager.train_classification_model(
    dataset=your_dataset,
    num_classes=12,
    model_config={"hidden_size": 256, "dropout_rate": 0.1},
    training_config={"num_train_epochs": 5, "learning_rate": 2e-5}
)
```

## ğŸ“‹ å®Œæ•´ç¯„ä¾‹

### é‹è¡Œåˆ†é¡ç¯„ä¾‹
```bash
cd examples
python classification_example.py
```

### é‹è¡Œå›é¥‹ä»‹é¢ç¯„ä¾‹
```bash
cd examples
python feedback_example.py
```

### é‹è¡Œå®Œæ•´æµç¨‹ç¯„ä¾‹
```bash
cd examples
python complete_pipeline_example.py
```

## ğŸ”§ é…ç½®é¸é …

### æ¨¡å‹é…ç½®
```python
model_config = {
    "base_model_name": "google/gemma-3-270m-it",
    "num_classes": 12,           # è¼¸å‡ºç¶­åº¦
    "hidden_size": 256,          # éš±è—å±¤å¤§å°
    "dropout_rate": 0.1,         # Dropout ç‡
    "use_lora": True,            # æ˜¯å¦ä½¿ç”¨ LoRA
    "lora_config": {
        "r": 16,                 # LoRA rank
        "lora_alpha": 32,        # LoRA alpha
        "lora_dropout": 0.1,     # LoRA dropout
        "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"]
    }
}
```

### è¨“ç·´é…ç½®
```python
training_config = {
    "num_train_epochs": 5,
    "per_device_train_batch_size": 4,
    "learning_rate": 2e-5,
    "warmup_steps": 100,
    "weight_decay": 0.01,
    "fp16": False,               # macOS MPS ç›¸å®¹æ€§
    "evaluation_strategy": "steps",
    "save_strategy": "steps",
}
```

## ğŸ“Š æç¤ºå·¥ç¨‹

æœ¬å¥—ä»¶æä¾›å¤šç¨®æç¤ºæ¨¡æ¿ï¼Œæå‡æ¨¡å‹å°é ˜åŸŸè³‡æ–™çš„ç†è§£ï¼š

### åˆ†é¡æç¤ºæ¨¡æ¿
```python
from hypersurrogatemodel import PromptTemplate

template = PromptTemplate("classification")
formatted_prompt = template.format_prompt(
    text="æ‚¨çš„è¼¸å…¥æ–‡å­—",
    template_type="classification"
)
```

### è‡ªå®šç¾©æç¤º
```python
# æ·»åŠ è‡ªå®šç¾©æç¤ºæ¨¡æ¿
template.add_custom_template(
    name="custom_domain",
    template="""
é ˜åŸŸï¼š{domain}
ä»»å‹™ï¼š{task_description}
è¼¸å…¥ï¼š{input_data}
åˆ†æï¼š...
è¼¸å‡ºï¼š"""
)
```

## ğŸ¯ ä¸»è¦ç‰¹æ€§

### 1. é«˜æ•ˆå¾®èª¿
- **LoRA æŠ€è¡“**: åƒ…è¨“ç·´ 1-2% çš„åƒæ•¸
- **è¨˜æ†¶é«”å„ªåŒ–**: å¤§å¹…é™ä½ GPU è¨˜æ†¶é«”éœ€æ±‚
- **å¿«é€Ÿæ”¶æ–‚**: å„ªåŒ–çš„å­¸ç¿’ç‡èª¿åº¦

### 2. æ™ºæ…§æç¤ºå·¥ç¨‹
- **é ˜åŸŸç‰¹åŒ–**: é‡å°ä¸åŒé ˜åŸŸçš„æç¤ºå„ªåŒ–
- **å¤šèªè¨€æ”¯æ´**: ä¸­è‹±æ–‡æ··åˆè™•ç†
- **ä¸Šä¸‹æ–‡å¢å¼·**: æå‡æ¨¡å‹ç†è§£èƒ½åŠ›

### 3. å®Œæ•´è©•ä¼°é«”ç³»
- **å¤šç¶­åº¦æŒ‡æ¨™**: æº–ç¢ºç‡ã€ç²¾ç¢ºç‡ã€å¬å›ç‡ã€F1 åˆ†æ•¸
- **éŒ¯èª¤åˆ†æ**: è©³ç´°çš„éŒ¯èª¤æ¨¡å¼åˆ†æ
- **è¦–è¦ºåŒ–å ±å‘Š**: æ··æ·†çŸ©é™£ã€æ€§èƒ½è¶¨å‹¢åœ–

### 4. å›é¥‹å¾ªç’°
- **ç”¨æˆ¶å›é¥‹æ”¶é›†**: æ”¯æ´åˆ†é¡å’Œç”Ÿæˆä»»å‹™å›é¥‹
- **æŒçºŒæ”¹é€²**: åŸºæ–¼å›é¥‹çš„æ¨¡å‹å„ªåŒ–
- **å“è³ªç›£æ§**: å¯¦æ™‚æ€§èƒ½ç›£æ§

### 5. ç”Ÿç”¢å°±ç·’
- **æ¨¡å‹ç‰ˆæœ¬æ§åˆ¶**: å®Œæ•´çš„æ¨¡å‹ç®¡ç†
- **é…ç½®ç®¡ç†**: éˆæ´»çš„é…ç½®ç³»çµ±
- **æ—¥èªŒè¨˜éŒ„**: è©³ç´°çš„è¨“ç·´å’Œæ¨ç†æ—¥èªŒ

## ğŸ“ˆ æ€§èƒ½åŸºæº–

åœ¨ 12 é¡åˆ†é¡ä»»å‹™ä¸Šçš„æ€§èƒ½è¡¨ç¾ï¼š

| æŒ‡æ¨™ | åŸºç¤æ¨¡å‹ | å¾®èª¿å¾Œ | æ”¹é€²å¹…åº¦ |
|------|----------|--------|----------|
| æº–ç¢ºç‡ | 0.672 | 0.896 | +33.4% |
| F1 åˆ†æ•¸ | 0.645 | 0.878 | +36.1% |
| ç²¾ç¢ºç‡ | 0.661 | 0.883 | +33.6% |
| å¬å›ç‡ | 0.672 | 0.896 | +33.4% |

## ğŸ› ï¸ é–‹ç™¼æŒ‡å—

### ä»£ç¢¼é¢¨æ ¼
æœ¬å°ˆæ¡ˆéµå¾ª PEP 8 æ¨™æº–å’Œ Google Python é¢¨æ ¼æŒ‡å—ï¼š
- ä½¿ç”¨ 4 ç©ºæ ¼ç¸®æ’
- è¡Œé•·åº¦ä¸è¶…é 88 å­—å…ƒ
- å®Œæ•´çš„ docstring æ–‡æª”
- é¡å‹æç¤ºæ”¯æ´

### è²¢ç»æŒ‡å—
1. Fork å°ˆæ¡ˆ
2. å‰µå»ºåŠŸèƒ½åˆ†æ”¯
3. ç·¨å¯«æ¸¬è©¦
4. æäº¤ Pull Request

### æ¸¬è©¦
```bash
# é‹è¡Œæ¸¬è©¦
python -m pytest tests/

# ä»£ç¢¼é¢¨æ ¼æª¢æŸ¥
black . --check
flake8 .
```

## ğŸ“ å°ˆæ¡ˆçµæ§‹

```
hypersurrogatemodel/
â”œâ”€â”€ hypersurrogatemodel/          # æ ¸å¿ƒå¥—ä»¶
â”‚   â”œâ”€â”€ __init__.py              # å¥—ä»¶åˆå§‹åŒ–
â”‚   â”œâ”€â”€ model.py                 # æ¨¡å‹å®šç¾©
â”‚   â”œâ”€â”€ dataset.py               # è³‡æ–™é›†è™•ç†
â”‚   â”œâ”€â”€ trainer.py               # è¨“ç·´ä»‹é¢
â”‚   â”œâ”€â”€ evaluator.py             # è©•ä¼°å’Œå›é¥‹
â”‚   â””â”€â”€ utils.py                 # å·¥å…·å‡½æ•¸
â”œâ”€â”€ examples/                    # ä½¿ç”¨ç¯„ä¾‹
â”‚   â”œâ”€â”€ classification_example.py
â”‚   â”œâ”€â”€ feedback_example.py
â”‚   â””â”€â”€ complete_pipeline_example.py
â”œâ”€â”€ requirements.txt             # ä¾è³´æ¸…å–®
â”œâ”€â”€ pyproject.toml              # å°ˆæ¡ˆé…ç½®
â””â”€â”€ README.md                   # å°ˆæ¡ˆæ–‡æª”
```

## ğŸ” å¸¸è¦‹å•é¡Œ

### Q: å¦‚ä½•è™•ç†è¨˜æ†¶é«”ä¸è¶³å•é¡Œï¼Ÿ
A: æ¸›å°‘ `batch_size`ï¼Œå¢åŠ  `gradient_accumulation_steps`ï¼Œæˆ–ä½¿ç”¨æ›´å°çš„ `max_length`ã€‚

### Q: è¨“ç·´é€Ÿåº¦å¤ªæ…¢æ€éº¼è¾¦ï¼Ÿ
A: ä½¿ç”¨ GPU åŠ é€Ÿï¼Œå¢åŠ  `batch_size`ï¼ˆåœ¨è¨˜æ†¶é«”å…è¨±çš„æƒ…æ³ä¸‹ï¼‰ï¼Œæˆ–æ¸›å°‘è¨“ç·´è³‡æ–™é‡ã€‚

### Q: å¦‚ä½•æ”¹å–„æ¨¡å‹æ€§èƒ½ï¼Ÿ
A: å¢åŠ è¨“ç·´è³‡æ–™ã€èª¿æ•´å­¸ç¿’ç‡ã€ä½¿ç”¨è³‡æ–™æ“´å¢ã€æˆ–å„ªåŒ–æç¤ºæ¨¡æ¿ã€‚

### Q: æ”¯æ´å“ªäº›è¨­å‚™ï¼Ÿ
A: æ”¯æ´ CUDA GPUã€Apple Silicon MPSã€å’Œ CPUã€‚é‡å° macOS é€²è¡Œäº†ç‰¹åˆ¥å„ªåŒ–ã€‚

## ğŸ“„ æˆæ¬Š

æœ¬å°ˆæ¡ˆæ¡ç”¨ MIT æˆæ¬Šæ¢æ¬¾ã€‚è©³è¦‹ [LICENSE](LICENSE) æ–‡ä»¶ã€‚

## ğŸ¤ è‡´è¬

- [Hugging Face Transformers](https://github.com/huggingface/transformers)
- [PEFT](https://github.com/huggingface/peft) 
- [Google Gemma](https://ai.google.dev/gemma)

## ğŸ“ æ”¯æ´

å¦‚æœ‰å•é¡Œæˆ–å»ºè­°ï¼Œè«‹ï¼š
- æäº¤ [Issue](https://github.com/your-repo/issues)
- ç™¼é€éƒµä»¶è‡³ï¼šsupport@your-domain.com
- æŸ¥çœ‹ [Wiki](https://github.com/your-repo/wiki) ç²å–æ›´å¤šæ–‡æª”

---

**Enhanced LLM Model** - è®“ AI æ›´æ™ºæ…§ï¼Œè®“é–‹ç™¼æ›´ç°¡å–® ğŸš€
