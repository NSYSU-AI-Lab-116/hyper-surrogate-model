# HyperSurrogateModel é…ç½®ç³»çµ±èªªæ˜

## ğŸ¯ æ¦‚è¦½

ä½¿ç”¨ YAML é…ç½®æ–‡ä»¶ä¾†æ§åˆ¶æ¨¡å‹çš„æ‰€æœ‰åƒæ•¸ï¼Œç°¡å–®ç›´è§€ï¼

## ğŸ“‹ æ”¯æ´çš„é…ç½®åƒæ•¸

### æ¨¡å‹é…ç½® (model)
- `pretrained_model`: é è¨“ç·´æ¨¡å‹åç¨± (å¦‚ "google/gemma-2-2b-it")
- `use_lora`: æ˜¯å¦ä½¿ç”¨LoRAå¾®èª¿ (true/false)
- `device`: é‹ç®—è¨­å‚™ ("cuda"/"cpu"/"auto")

### ç”Ÿæˆé…ç½® (generation)
- `max_new_tokens`: æœ€å¤§ç”Ÿæˆtokenæ•¸ (é è¨­: 50)
- `temperature`: æº«åº¦åƒæ•¸ï¼Œæ§åˆ¶éš¨æ©Ÿæ€§ (é è¨­: 0.7)
- `do_sample`: æ˜¯å¦ä½¿ç”¨æ¡æ¨£ (é è¨­: true)
- `top_k`: Top-kæ¡æ¨£åƒæ•¸ (é è¨­: 64)
- `top_p`: Top-p (nucleus) æ¡æ¨£åƒæ•¸ (é è¨­: 0.95)
- `repetition_penalty`: é‡è¤‡æ‡²ç½° (é è¨­: 1.1)
- `length_penalty`: é•·åº¦æ‡²ç½° (é è¨­: 1.0)
- `num_beams`: Beam searchæ•¸é‡ (é è¨­: 1)

### LoRAé…ç½® (lora)
- `r`: LoRA rank (é è¨­: 16)
- `lora_alpha`: LoRA alpha (é è¨­: 32)
- `lora_dropout`: LoRA dropout (é è¨­: 0.1)
- `target_modules`: ç›®æ¨™æ¨¡çµ„åˆ—è¡¨

### è¨“ç·´é…ç½® (training)
- `batch_size`: æ‰¹æ¬¡å¤§å° (é è¨­: 8)
- `learning_rate`: å­¸ç¿’ç‡ (é è¨­: 2e-5)
- `num_epochs`: è¨“ç·´è¼ªæ•¸ (é è¨­: 3)
- ç­‰ç­‰...

## ğŸ”§ ä½¿ç”¨æ–¹æ³•

### ä¸»é…ç½®æ–‡ä»¶: config.yaml

ç·¨è¼¯ `config.yaml` æ–‡ä»¶ä¾†è¨­å®šæ‚¨çš„åƒæ•¸ï¼š

```yaml
# æ¨¡å‹è¨­å®š
model:
  pretrained_model: "microsoft/DialoGPT-medium"
  use_lora: true
  device: "auto"

# ç”Ÿæˆåƒæ•¸
generation:
  max_new_tokens: 150
  temperature: 0.9
  top_k: 40
  top_p: 0.8
  do_sample: true

# LoRAè¨­å®š
lora:
  r: 16
  lora_alpha: 32
  lora_dropout: 0.1
  target_modules:
    - "q_proj"
    - "v_proj"
    - "k_proj"
    - "o_proj"

# è¨“ç·´åƒæ•¸
training:
  batch_size: 4
  learning_rate: 2e-5
  num_epochs: 3
```

### ä½¿ç”¨æ¨¡å‹

```python
from hypersurrogatemodel import TrainableLLM

# è‡ªå‹•è®€å– config.yaml
model = TrainableLLM()

# æˆ–æŒ‡å®šè‡ªå®šç¾©é…ç½®æ–‡ä»¶
model = TrainableLLM()  # æœƒè‡ªå‹•ä½¿ç”¨ config.yaml
```

## ğŸ“Š é…ç½®æ–¹å¼

**å”¯ä¸€é…ç½®ä¾†æº**: YAML é…ç½®æ–‡ä»¶ (`config.yaml`)
- ç°¡å–®ç›´è§€çš„éšå±¤å¼çµæ§‹
- æ”¯æ´ä¸­æ–‡è¨»è§£
- é¡å‹å®‰å…¨ï¼Œä¸éœ€è¦å­—ä¸²è½‰æ›
- ç‰ˆæœ¬æ§åˆ¶å‹å¥½

## ğŸ§ª æ¸¬è©¦é…ç½®

é‹è¡Œç¤ºä¾‹ç¨‹å¼ä¾†æ¸¬è©¦é…ç½®ç³»çµ±ï¼š

```bash
cd /home/alvin/hyper-surrogate-model
uv run python example_config_usage.py
```

## ğŸ“ é…ç½®æ–‡ä»¶ç¯„ä¾‹

å®Œæ•´çš„ `config.yaml` ç¯„ä¾‹ï¼š

```yaml
# Hyper Surrogate Model Configuration
model:
  pretrained_model: "google/gemma-2-2b-it"
  use_lora: true
  device: "auto"

generation:
  max_new_tokens: 50
  temperature: 0.7
  do_sample: true
  top_k: 64
  top_p: 0.95
  repetition_penalty: 1.1
  length_penalty: 1.0
  num_beams: 1

training:
  batch_size: 8
  learning_rate: 2e-5
  num_epochs: 3
  warmup_steps: 100
  weight_decay: 0.01
  fp16: false

lora:
  r: 16
  lora_alpha: 32
  lora_dropout: 0.1
  target_modules:
    - "q_proj"
    - "v_proj"
    - "k_proj"
    - "o_proj"

logging:
  level: "INFO"
  use_wandb: false
  wandb_project: "hypersurrogatemodel"
  save_files: true
  output_dir: "./results"

dataset:
  max_length: 512
  padding: "max_length"
  truncation: true
  template_type: "generation"

comparison:
  method: "similarity"
  batch_size: 256
  similarity_threshold: 0.8
  tuning_strategy: "error_focused"
```

## ğŸ‰ å®Œæˆï¼

ç¾åœ¨æ‚¨åªéœ€è¦ç·¨è¼¯ `config.yaml` å°±èƒ½æ§åˆ¶æ‰€æœ‰æ¨¡å‹åƒæ•¸ï¼Œç°¡å–®æ˜ç­ï¼
