"""
æ¸¬è©¦æ¯”å°èª¿å„ªåŠŸèƒ½çš„ç°¡å–®ç¤ºä¾‹
"""

import json
import tempfile
from pathlib import Path

def test_comparison_tuner():
    """æ¸¬è©¦ComparisonTuneråŸºæœ¬åŠŸèƒ½"""
    print("ğŸ§ª é–‹å§‹æ¸¬è©¦ComparisonTuneråŠŸèƒ½...")
    
    try:
        from hypersurrogatemodel import TrainableLLM, ComparisonTuner
        print("âœ… æ¨¡çµ„å°å…¥æˆåŠŸ")
        
        # å‰µå»ºæ¸¬è©¦æ•¸æ“šé›†
        test_data = [
            {
                "text": "é¸æ“‡é©åˆç©ºæ°£å‹•åŠ›å­¸æ¨¡æ“¬çš„ä»£ç†æ¨¡å‹",
                "answer": "ç¥ç¶“ç¶²è·¯"
            },
            {
                "text": "ç°¡å–®ç·šæ€§é—œä¿‚å»ºæ¨¡",
                "answer": "ç·šæ€§å›æ­¸"
            }
        ]
        
        # å‰µå»ºè‡¨æ™‚æ•¸æ“šé›†æ–‡ä»¶
        with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False, encoding='utf-8') as f:
            json.dump(test_data, f, ensure_ascii=False, indent=2)
            temp_dataset_path = f.name
        
        print(f"âœ… æ¸¬è©¦æ•¸æ“šé›†å‰µå»ºæˆåŠŸ: {temp_dataset_path}")
        
        # åˆå§‹åŒ–æ¨¡å‹ï¼ˆä½¿ç”¨å°æ¨¡å‹ä»¥ç¯€çœè³‡æºï¼‰
        print("ğŸ¤– åˆå§‹åŒ–æ¨¡å‹...")
        model = TrainableLLM(
            base_model_name="google/gemma-3-270m-it",
            use_lora=True,
        )
        
        print("âœ… æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ")
        
        # åˆå§‹åŒ–ComparisonTuner
        print("ğŸ”§ åˆå§‹åŒ–ComparisonTuner...")
        tuner = ComparisonTuner(
            model=model,
            tokenizer=model.get_tokenizer(),
            output_dir="./test_results",
            use_wandb=False
        )
        
        print("âœ… ComparisonTuneråˆå§‹åŒ–æˆåŠŸ")
        
        # æ¸¬è©¦æ•¸æ“šé›†è¼‰å…¥å’Œæ¯”å°
        print("ğŸ“Š æ¸¬è©¦æ•¸æ“šé›†è¼‰å…¥å’Œæ¯”å°...")
        try:
            comparison_results = tuner.load_and_compare_dataset(
                dataset_path=temp_dataset_path,
                text_column="text",
                answer_column="answer", 
                task_type="generation",
                comparison_method="similarity"
            )
            
            print("âœ… æ•¸æ“šé›†æ¯”å°æˆåŠŸ")
            print(f"   ç¸½æ¨£æœ¬æ•¸: {comparison_results['overall_metrics']['total_samples']}")
            print(f"   æº–ç¢ºç‡: {comparison_results['overall_metrics']['accuracy']:.3f}")
            print(f"   å¹³å‡ç›¸ä¼¼åº¦: {comparison_results['overall_metrics']['average_similarity']:.3f}")
            
        except Exception as e:
            print(f"âŒ æ•¸æ“šé›†æ¯”å°å¤±æ•—: {e}")
            return False
        
        # æ¸…ç†è‡¨æ™‚æ–‡ä»¶
        Path(temp_dataset_path).unlink()
        print("ğŸ§¹ æ¸…ç†è‡¨æ™‚æ–‡ä»¶å®Œæˆ")
        
        print("ğŸ‰ æ‰€æœ‰æ¸¬è©¦é€šéï¼ComparisonTuneråŠŸèƒ½æ­£å¸¸")
        return True
        
    except ImportError as e:
        print(f"âŒ å°å…¥å¤±æ•—: {e}")
        return False
    except Exception as e:
        print(f"âŒ æ¸¬è©¦å¤±æ•—: {e}")
        return False

if __name__ == "__main__":
    success = test_comparison_tuner()
    if success:
        print("\\nâœ¨ ComparisonTuneråŠŸèƒ½æ¸¬è©¦æˆåŠŸï¼")
        print("\\næ‚¨ç¾åœ¨å¯ä»¥ä½¿ç”¨ä»¥ä¸‹åŠŸèƒ½ï¼š")
        print("1. è¼‰å…¥æ•¸æ“šé›†ä¸¦èˆ‡LLMé æ¸¬æ¯”å°")
        print("2. åˆ†æé æ¸¬å·®ç•°å’ŒéŒ¯èª¤æ¨¡å¼") 
        print("3. åŸºæ–¼å·®ç•°é€²è¡Œè‡ªé©æ‡‰èª¿å„ª")
        print("4. è¿½è¹¤æ”¹é€²æ•ˆæœ")
        print("\\næŸ¥çœ‹ examples/quick_start_comparison.py ç²å–ä½¿ç”¨ç¤ºä¾‹")
    else:
        print("\\nâŒ æ¸¬è©¦å¤±æ•—ï¼Œè«‹æª¢æŸ¥éŒ¯èª¤è¨Šæ¯")
